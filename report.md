บทวิเคราะห์เชิงลึกเกี่ยวกับระบบนิเวศ SWE-bench และ SWE-smith สำหรับการพัฒนาโมเดลภาษา (LLM)
======================================================================

บทสรุปผู้บริหาร
-----------------
รายงานฉบับนี้ทำหน้าที่เป็นคู่มือเชิงเทคนิคฉบับสมบูรณ์เกี่ยวกับระบบนิเวศ SWE-bench ซึ่งได้กลายเป็นมาตรฐานกลางสำหรับการประเมินความสามารถด้านวิศวกรรมซอฟต์แวร์ของโมเดลภาษา-ขนาดใหญ่ (LLM) และเอเจนต์ AI อัตโนมัติ รายงานจะอธิบายบทบาทขององค์ประกอบหลักแต่ละส่วน โครงสร้างข้อมูล ขั้นตอนการประเมิน ตลอดจนข้อเสนอแนะเชิงกลยุทธ์สำหรับผู้พัฒนาและนักวิจัย

ภาพรวม (Overview)
------------------
SWE-bench คือชุดข้อมูลทดสอบ (benchmark) ที่รวบรวมปัญหาจริงจากโครงการโอเพ่นซอร์สยอดนิยมบน GitHub เพื่อตรวจสอบว่า AI สามารถสร้าง "แพตช์" โค้ดที่แก้ไขบั๊กหรือทำตามคำขอคุณลักษณะได้สำเร็จหรือไม่ ในขณะที่ SWE-smith เป็นเครื่องมือรุ่นใหม่ที่ออกแบบมาเพื่อสร้างข้อมูลฝึก (training data) ปริมาณมากในลักษณะคล้าย SWE-bench แต่สำหรับใช้ฝึกสอนโมเดล ไม่ใช่ใช้ทดสอบ  การเข้าใจความแตกต่างพื้นฐานนี้เป็นสิ่งสำคัญ

ประเด็นสำคัญ (Key Findings)
---------------------------
1. **SWE-bench** = ชุดทดสอบ / **SWE-smith** = เครื่องมือสร้างข้อมูลฝึก ทั้งสองส่วนทำงานเสริมกันแต่ไม่เหมือนกัน  
2. SWE-bench เป็นศูนย์กลางการวิจัยที่มีการพัฒนาอย่างรวดเร็ว ไม่ใช่ชุดข้อมูลคงที่  
3. SWE-agent แสดงให้เห็นว่าเวิร์กโฟลว์แบบ "เอเจนต์มีเครื่องมือ" เหนือกว่าการสร้างแพตช์แบบครั้งเดียว  
4. SWE-smith แก้ปัญหาการขาดแคลนข้อมูลฝึกด้วยแนวคิด "สร้างสภาพแวดล้อมก่อน แล้วค่อยสังเคราะห์บั๊ก"  
5. มีเวอร์ชันของ SWE-bench หลายชุด (Full, Lite, Verified, Multimodal ฯลฯ) ควรเลือกให้ตรงวัตถุประสงค์

โครงสร้างระบบนิเวศ SWE-bench
-----------------------------
### 1. SWE-bench – สนามทดสอบ
* ภารกิจ: ให้โมเดลสร้างแพตช์โค้ดที่ทำให้เทสต์ที่เคยล้มเหลว (FAIL_TO_PASS) ผ่าน พร้อมไม่ทำให้เทสต์อื่นพัง (PASS_TO_PASS)
* ขนาดชุดข้อมูลดั้งเดิม: 2,294 อินสแตนซ์ จาก 12 รีโป Python
* ใช้ Docker แยกสิ่งแวดล้อมต่ออินสแตนซ์ → ต้องการทรัพยากรสูง (≥ 120 GB disk, 16 GB RAM)

### 2. SWE-agent – ตัวท้าชิงระดับสูงสุด
* ใช้ LLM (เช่น GPT-4o, Claude 3) + คำสั่งเสมือนมนุษย์ (ค้นหา, เปิดไฟล์, แก้ไข, รันเทสต์)
* ทำงานแบบวนซ้ำ วิเคราะห์-เขียนโค้ด-รันเทสต์ จนเทสต์ผ่าน

### 3. SWE-smith – โรงงานผลิตข้อมูลฝึก
* พลิกกระบวนท่า: สร้าง Docker ที่เสถียรก่อน → สังเคราะห์บั๊กจำนวนมากภายใน   
* ลดขนาดที่เก็บจาก TB → GB และลดแรงงานมนุษย์อย่างมาก   
* ออกชุด 50k อินสแตนซ์ + โมเดลเปิดเผย 32B ("SWE-agent-LM-32B") ที่ฝึกบนข้อมูลนี้

ชุดข้อมูลย่อย (Dataset Variants)
--------------------------------
| ชุด | HuggingFace ID | ขนาด (test) | ใช้งานหลัก |
|-----|----------------|-------------|-------------|
| SWE-bench (Full) | princeton-nlp/SWE-bench | 2,294 | ประเมินขั้นสุดท้าย |
| SWE-bench Lite | princeton-nlp/SWE-bench_Lite | 534 | ทดสอบ/ดีบั๊กอย่างรวดเร็ว |
| SWE-bench Verified | princeton-nlp/SWE-bench_Verified | 500 | บรรทัดฐานทองคำ (human-verified) |
| SWE-bench Multimodal | princeton-nlp/SWE-bench_Multimodal | 500 | ทดสอบความเข้าใจภาพ-โค้ด |

สคีมาอินสแตนซ์ข้อมูล (ย่อ)
----------------------------
* `instance_id`: รหัสเฉพาะ (e.g. django__django-11477)  
* `repo`: ชื่อรีโป GitHub  
* `problem_statement`: ข้อความ issue  
* `base_commit`: คอมมิตก่อนแพตช์  
* `patch`: แพตช์ของมนุษย์ (คำตอบอ้างอิง)  
* `test_patch`: แพตช์เทสต์ใหม่  
* `FAIL_TO_PASS`: รายชื่อเทสต์ที่จะต้องเปลี่ยนจาก fail → pass  
* `PASS_TO_PASS`: เทสต์ต้องผ่านเสมอ  

ขั้นตอนการประเมิน (Evaluation Protocol)
---------------------------------------
1. สร้าง Docker image ต่อหนึ่งอินสแตนซ์   
2. ใช้เทสต์เป็นตัววัด ไม่สนใจว่าแพตช์เหมือนต้นฉบับหรือไม่ ขอให้ฟังก์ชันถูกต้อง   
3. ส่งผลลัพธ์เป็น JSONL (`instance_id`, `model_name`, `patch`)  
4. นักวิจัยตั้งข้อสังเกตจุดอ่อน เช่น เทสต์ไม่รัดกุม, พฤติกรรมต่างจากแพตช์มนุษย์, prompt รั่วข้อมูล

กลยุทธ์การใช้ SWE-bench / SWE-smith
------------------------------------
### การฝึกโมเดล
* เริ่มจาก train split ของ SWE-bench สำหรับต้นแบบ  
* ใช้ SWE-smith + ชุด 50k อินสแตนซ์ เมื่อจำเป็นต้องสเกล  
* หลีกเลี่ยงการรั่วไหลของข้อมูลทดสอบเข้าสู่ชุดฝึก

### การประเมินโมเดล
* ใช้ SWE-bench Lite เพื่อวนปรับจูนอย่างรวดเร็ว  
* ใช้ SWE-bench Verified เพื่อรายงานผลอย่างเป็นทางการ  
* เตรียมทรัพยากร (disk, CPU, RAM) หรือใช้บริการคลาวด์ sb-cli/Modal

ข้อเสนอแนะในการตีความผลลัพธ์
------------------------------
* คะแนน "Resolved" สูงเป็นสัญญาณบวก แต่ต้องพิจารณาคุณภาพเทสต์และพฤติกรรมโค้ดด้วย  
* ติดตาม leaderboard และชุดข้อมูลใหม่ (เช่น SWE-bench-Live, Multi-SWE-bench) อย่างสม่ำเสมอ

สรุปท้าย
---------
ระบบนิเวศ SWE-bench ได้เปลี่ยนสนามประลอง AI จากโจทย์อัลกอริทึมแบบแยกส่วน ไปสู่ปัญหาวิศวกรรมซอฟต์แวร์จริงที่ซับซ้อน พร้อมกันนั้น SWE-smith ได้ปลดล็อกการสร้างข้อมูลฝึกขนาดใหญ่ ให้ชุมชนโอเพ่นซอร์สสามารถพัฒนาโมเดลเฉพาะทางได้เอง ความเข้าใจบทบาทของทั้งสองเครื่องมือจะช่วยให้ผู้ปฏิบัติงานออกแบบกลยุทธ์การฝึกและประเมินโมเดลได้อย่างมีประสิทธิผลมากขึ้น

> หมายเหตุ: เพื่อลดความยาว รายงานฉบับแปลนี้ได้ละเว้นตัวอย่าง 100 รายการของชุดฝึกและชุดทดสอบ ซึ่งสามารถดูได้จากต้นฉบับภาษาอังกฤษ